{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nimport string\nfrom string import digits\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport re\n\nimport pandas as pd \nimport numpy as np\nimport seaborn as sns\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nfrom keras.layers import Input, LSTM, Embedding, Dense\nfrom keras.models import Model","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df= pd.read_csv('/kaggle/input/nepali-data/Eng_Nep.csv')\ndf.head()","execution_count":2,"outputs":[{"output_type":"execute_result","execution_count":2,"data":{"text/plain":"                                    english_sentence  \\\n0  politicians do not have permission to do what ...   \n1         I'd like to tell you about one such child,   \n2  This percentage is even greater than the perce...   \n3  what we really mean is that they're bad at not...   \n4  .The ending portion of these Vedas is called U...   \n\n                                     nepali_sentence  \n0         राजनीतिज्ञ गरिन आवश्यक के गर्न अनुमति छैन।  \n1             म एक यस्तो बच्चा बारेमा बताउन चाहन्छु,  \n2          यो प्रतिशत भारत प्रतिशत भन्दा पनि ठूलो छ।  \n3  हामी साँच्चै के मतलब तिनीहरूले ध्यान छैन मा हु...  \n4             यी Vedas को अन्त्य भाग उपनिषद् भनिन्छ।  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>english_sentence</th>\n      <th>nepali_sentence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>politicians do not have permission to do what ...</td>\n      <td>राजनीतिज्ञ गरिन आवश्यक के गर्न अनुमति छैन।</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I'd like to tell you about one such child,</td>\n      <td>म एक यस्तो बच्चा बारेमा बताउन चाहन्छु,</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>This percentage is even greater than the perce...</td>\n      <td>यो प्रतिशत भारत प्रतिशत भन्दा पनि ठूलो छ।</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>what we really mean is that they're bad at not...</td>\n      <td>हामी साँच्चै के मतलब तिनीहरूले ध्यान छैन मा हु...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>.The ending portion of these Vedas is called U...</td>\n      <td>यी Vedas को अन्त्य भाग उपनिषद् भनिन्छ।</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop_duplicates(inplace=True)\ndf['nepali']=df['nepali_sentence'].apply(lambda x: True if re.search('[A-Za-z]',x) else False )\ndf.drop(df[df['nepali'] == True].index, inplace=True)","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"                                    english_sentence  \\\n0  politicians do not have permission to do what ...   \n1         I'd like to tell you about one such child,   \n2  This percentage is even greater than the perce...   \n3  what we really mean is that they're bad at not...   \n5  The then Governor of Kashmir resisted transfer...   \n\n                                     nepali_sentence  nepali  \n0         राजनीतिज्ञ गरिन आवश्यक के गर्न अनुमति छैन।   False  \n1             म एक यस्तो बच्चा बारेमा बताउन चाहन्छु,   False  \n2          यो प्रतिशत भारत प्रतिशत भन्दा पनि ठूलो छ।   False  \n3  हामी साँच्चै के मतलब तिनीहरूले ध्यान छैन मा हु...   False  \n5  त कश्मीर को राज्यपाल स्थानान्तरण प्रतिरोध, तर ...   False  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>english_sentence</th>\n      <th>nepali_sentence</th>\n      <th>nepali</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>politicians do not have permission to do what ...</td>\n      <td>राजनीतिज्ञ गरिन आवश्यक के गर्न अनुमति छैन।</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I'd like to tell you about one such child,</td>\n      <td>म एक यस्तो बच्चा बारेमा बताउन चाहन्छु,</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>This percentage is even greater than the perce...</td>\n      <td>यो प्रतिशत भारत प्रतिशत भन्दा पनि ठूलो छ।</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>what we really mean is that they're bad at not...</td>\n      <td>हामी साँच्चै के मतलब तिनीहरूले ध्यान छैन मा हु...</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>The then Governor of Kashmir resisted transfer...</td>\n      <td>त कश्मीर को राज्यपाल स्थानान्तरण प्रतिरोध, तर ...</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['english_sentence'] = df['english_sentence'].apply(lambda x:x.lower())\ndf['nepali_sentence'] = df['nepali_sentence'].apply(lambda x:x.lower())","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove quotes\ndf['english_sentence']=df['english_sentence'].apply(lambda x: re.sub(\"'\", '', x))\ndf['nepali_sentence']=df['nepali_sentence'].apply(lambda x: re.sub(\"'\", '', x))","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"exclude = set(string.punctuation) # Set of all special characters\n# Remove all the special characters\ndf['english_sentence']=df['english_sentence'].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\ndf['nepali_sentence']=df['nepali_sentence'].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove all numbers from text\nremove_digits = str.maketrans('', '', digits)\ndf['english_sentence']=df['english_sentence'].apply(lambda x: x.translate(remove_digits))\ndf['nepali_sentence']=df['nepali_sentence'].apply(lambda x: x.translate(remove_digits))\n\ndf['nepali_sentence'] = df['nepali_sentence'].apply(lambda x: re.sub(\"[२३०८१५७९४६]\", \"\", x))\n\n# Remove extra spaces\ndf['english_sentence']=df['english_sentence'].apply(lambda x: x.strip())\ndf['nepali_sentence']=df['nepali_sentence'].apply(lambda x: x.strip())\ndf['english_sentence']=df['english_sentence'].apply(lambda x: re.sub(\" +\", \" \", x))\ndf['nepali_sentence']=df['nepali_sentence'].apply(lambda x: re.sub(\" +\", \" \", x))","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add start and end tokens to target sequences\ndf['nepali_sentence'] = df['nepali_sentence'].apply(lambda x : 'START_ '+ x + ' _END')\n","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Get English and nepali Vocabulary\nall_eng_words=set()\nfor eng in df['english_sentence']:\n    for word in eng.split():\n        if word not in all_eng_words:\n            all_eng_words.add(word)\n\nall_nepali_words=set()\nfor nep in df['nepali_sentence']:\n    for word in nep.split():\n        if word not in all_nepali_words:\n            all_nepali_words.add(word)","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['length_eng_sentence']=df['english_sentence'].apply(lambda x:len(x.split(\" \")))\ndf['length_nep_sentence']=df['nepali_sentence'].apply(lambda x:len(x.split(\" \")))","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=df[df['length_eng_sentence']<=20]\ndf=df[df['length_nep_sentence']<=20]","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"maximum length of NepaliSentence \",max(df['length_nep_sentence']))\nprint(\"maximum length of English Sentence \",max(df['length_eng_sentence']))","execution_count":15,"outputs":[{"output_type":"stream","text":"maximum length of NepaliSentence  20\nmaximum length of English Sentence  20\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_length_src=max(df['length_nep_sentence'])\nmax_length_tar=max(df['length_eng_sentence'])\n# max_length_src,max_length_tar","execution_count":17,"outputs":[{"output_type":"execute_result","execution_count":17,"data":{"text/plain":"(20, 20)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_words = sorted(list(all_eng_words))\ntarget_words = sorted(list(all_nepali_words))\nnum_encoder_tokens = len(all_eng_words)\nnum_decoder_tokens = len(all_nepali_words)\nnum_encoder_tokens, num_decoder_tokens","execution_count":18,"outputs":[{"output_type":"execute_result","execution_count":18,"data":{"text/plain":"(11198, 10821)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_decoder_tokens += 1 #for zero padding\n\n# num_encoder_tokens = num_encoder_tokens + 1 ","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_token_index = dict([(word, i+1) for i, word in enumerate(input_words)])\ntarget_token_index = dict([(word, i+1) for i, word in enumerate(target_words)])","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reverse_input_char_index = dict((i, word) for word, i in input_token_index.items())\nreverse_target_char_index = dict((i, word) for word, i in target_token_index.items())","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X, y = df['english_sentence'], df['nepali_sentence']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2,random_state=42)\nX_train.shape, X_test.shape","execution_count":22,"outputs":[{"output_type":"execute_result","execution_count":22,"data":{"text/plain":"((7942,), (1986,))"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# encoder_input_data = np.zeros(\n#     (len(df.english_sentence), max_length_src),\n#     dtype='float32')\n# decoder_input_data = np.zeros(\n#     (len(df.nepali_sentence), max_length_tar),\n#     dtype='float32')\n# decoder_target_data = np.zeros(\n#     (len(df.nepali_sentence), max_length_tar, num_decoder_tokens),\n#     dtype='float32')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for i, (input_text, target_text) in enumerate(zip(df.english_sentence, df.nepali_sentence)):\n#     for t, word in enumerate(input_text.split()):\n#         encoder_input_data[i, t] = input_token_index[word]\n#     for t, word in enumerate(target_text.split()):\n#         # decoder_target_data is ahead of decoder_input_data by one timestep\n#         decoder_input_data[i, t] = target_token_index[word]\n#         if t > 0:\n#             # decoder_target_data will be ahead by one timestep\n#             # and will not include the start character.\n#             decoder_target_data[i, t - 1, target_token_index[word]] = 1.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_batch(X = X_train, y = y_train, batch_size = 128):\n    ''' Generate a batch of data '''\n    while True:\n        for j in range(0, len(X), batch_size):\n            encoder_input_data = np.zeros((batch_size, max_length_src),dtype='float32')\n            decoder_input_data = np.zeros((batch_size, max_length_tar),dtype='float32')\n            decoder_target_data = np.zeros((batch_size, max_length_tar, num_decoder_tokens),dtype='float32')\n            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n                for t, word in enumerate(input_text.split()):\n                    encoder_input_data[i, t] = input_token_index[word] # encoder input seq\n                for t, word in enumerate(target_text.split()):\n                    if t<len(target_text.split())-1:\n                        decoder_input_data[i, t] = target_token_index[word] # decoder input seq\n                    if t>0:\n                        # decoder target sequence (one hot encoded)\n                        # does not include the START_ token\n                        # Offset by one timestep\n                        decoder_target_data[i, t - 1, target_token_index[word]] = 1.\n            yield([encoder_input_data, decoder_input_data], decoder_target_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_size = 50","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Input(shape=(None,))","execution_count":31,"outputs":[{"output_type":"execute_result","execution_count":31,"data":{"text/plain":"<tf.Tensor 'input_1:0' shape=(None, None) dtype=float32>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder_inputs = Input(shape=(None,))\nen_x=  Embedding(num_encoder_tokens, embedding_size)(encoder_inputs)\nencoder = LSTM(50, return_state=True)\nencoder_outputs, state_h, state_c = encoder(en_x)\n# We discard `encoder_outputs` and only keep the states.\nencoder_states = [state_h, state_c]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set up the decoder, using `encoder_states` as initial state.\ndecoder_inputs = Input(shape=(None,))\n\ndex=  Embedding(num_decoder_tokens, embedding_size)\n\nfinal_dex= dex(decoder_inputs)\n\n\ndecoder_lstm = LSTM(50, return_sequences=True, return_state=True)\n\ndecoder_outputs, _, _ = decoder_lstm(final_dex,\n                                     initial_state=encoder_states)\n\ndecoder_dense = Dense(num_decoder_tokens, activation='softmax')\n\ndecoder_outputs = decoder_dense(decoder_outputs)\n\nmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_samples = len(X_train)\nval_samples = len(X_test)\nbatch_size = 128\nepochs = 20","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit_generator(generator = generate_batch(X_train, y_train, batch_size = batch_size),\n                    steps_per_epoch = train_samples//batch_size,\n                    epochs=epochs,\n                    validation_data = generate_batch(X_test, y_test, batch_size = batch_size),\n                    validation_steps = val_samples//batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n#           batch_size=128,\n#           epochs=20,\n#           validation_split=0.05)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder_model = Model(encoder_inputs, encoder_states)\nencoder_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder_state_input_h = Input(shape=(50,))\ndecoder_state_input_c = Input(shape=(50,))\ndecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n\nfinal_dex2= dex(decoder_inputs)\n\ndecoder_outputs2, state_h2, state_c2 = decoder_lstm(final_dex2, initial_state=decoder_states_inputs)\ndecoder_states2 = [state_h2, state_c2]\ndecoder_outputs2 = decoder_dense(decoder_outputs2)\ndecoder_model = Model(\n    [decoder_inputs] + decoder_states_inputs,\n    [decoder_outputs2] + decoder_states2)\n\n# Reverse-lookup token index to decode sequences back to\n# something readable.\nreverse_input_char_index = dict(\n    (i, char) for char, i in input_token_index.items())\nreverse_target_char_index = dict(\n    (i, char) for char, i in target_token_index.items())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def decode_sequence(input_seq):\n    # Encode the input as state vectors.\n    states_value = encoder_model.predict(input_seq)\n    # Generate empty target sequence of length 1.\n    target_seq = np.zeros((1,1))\n    # Populate the first character of target sequence with the start character.\n    target_seq[0, 0] = target_token_index['START_']\n\n    # Sampling loop for a batch of sequences\n    # (to simplify, here we assume a batch of size 1).\n    stop_condition = False\n    decoded_sentence = ''\n    while not stop_condition:\n        output_tokens, h, c = decoder_model.predict(\n            [target_seq] + states_value)\n\n        # Sample a token\n        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n        sampled_char = reverse_target_char_index[sampled_token_index]\n        decoded_sentence += ' '+sampled_char\n\n        # Exit condition: either hit max length\n        # or find stop character.\n        if (sampled_char == '_END' or\n           len(decoded_sentence) > 52):\n            stop_condition = True\n\n        # Update the target sequence (of length 1).\n        target_seq = np.zeros((1,1))\n        target_seq[0, 0] = sampled_token_index\n\n        # Update states\n        states_value = [h, c]\n\n    return decoded_sentence","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for seq_index in [14077,10000,5000,1000,18000]:\n#     input_seq = encoder_input_data[seq_index: seq_index + 1]\n#     decoded_sentence = decode_sequence(input_seq)\n#     print('-')\n#     print('Input sentence:', df.english_sentence[seq_index: seq_index + 1])\n#     print('Decoded sentence:', decoded_sentence)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_gen = generate_batch(X_train, y_train, batch_size = 1)\nk=-1\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k+=1\n(input_seq, actual_output), _ = next(train_gen)\ndecoded_sentence = decode_sequence(input_seq)\nprint('Input English sentence:', X_train[k:k+1].values[0])\nprint('Actual Nepali Translation:', y_train[k:k+1].values[0][6:-4])\nprint('Predicted Nepali Translation:', decoded_sentence[:-4])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k+=1\n(input_seq, actual_output), _ = next(train_gen)\ndecoded_sentence = decode_sequence(input_seq)\nprint('Input English sentence:', X_train[k:k+1].values[0])\nprint('Actual Nepali Translation:', y_train[k:k+1].values[0][6:-4])\nprint('Predicted Nepali Translation:', decoded_sentence[:-4])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k+=1\n(input_seq, actual_output), _ = next(train_gen)\ndecoded_sentence = decode_sequence(input_seq)\nprint('Input English sentence:', X_train[k:k+1].values[0])\nprint('Actual Nepali Translation:', y_train[k:k+1].values[0][6:-4])\nprint('Predicted Nepali Translation:', decoded_sentence[:-4])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k+=1\n(input_seq, actual_output), _ = next(train_gen)\ndecoded_sentence = decode_sequence(input_seq)\nprint('Input English sentence:', X_train[k:k+1].values[0])\nprint('Actual Nepali Translation:', y_train[k:k+1].values[0][6:-4])\nprint('Predicted Nepali Translation:', decoded_sentence[:-4])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k+=1\n(input_seq, actual_output), _ = next(train_gen)\ndecoded_sentence = decode_sequence(input_seq)\nprint('Input English sentence:', X_train[k:k+1].values[0])\nprint('Actual Nepali Translation:', y_train[k:k+1].values[0][6:-4])\nprint('Predicted Nepali Translation:', decoded_sentence[:-4])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k+=1\n(input_seq, actual_output), _ = next(train_gen)\ndecoded_sentence = decode_sequence(input_seq)\nprint('Input English sentence:', X_train[k:k+1].values[0])\nprint('Actual Nepali Translation:', y_train[k:k+1].values[0][6:-4])\nprint('Predicted Nepali Translation:', decoded_sentence[:-4])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}